# -*- coding: utf-8 -*-
"""squaredmedian.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VRTsBk1yDaSOzFBY1i6ZrMcj4ib7OphW

Spectrogram of Each Chnnel
"""

import librosa
import numpy as np
import matplotlib.pyplot as plt

# File path
file = "/content/DroneSound_noMuff.wav"

# Load audio
y, sr = librosa.load(file, sr=None, mono=False)

# Ensure y is 2D (channels x samples)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = y.shape[0]
print(f"Generating spectrograms for {num_channels} channels")

# Parameters for the spectrogram
n_fft = 2048       # FFT window size
hop_length = 512   # Number of samples between successive FFTs

# --- Loop through each channel and plot its spectrogram ---
for i in range(num_channels):
    # Calculate the power spectrogram for the current channel
    S = np.abs(librosa.stft(y[i], n_fft=n_fft, hop_length=hop_length))
    S_db = librosa.amplitude_to_db(S, ref=np.max)

    # Plot the spectrogram
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(S_db, sr=sr, hop_length=hop_length, x_axis='time', y_axis='hz', cmap='viridis')
    plt.colorbar(label='Amplitude (dB)')
    plt.title(f'Spectrogram of Channel {i+1}')
    plt.tight_layout()
    plt.show()

"""In terms of amplitude (0-1)

"""

import librosa
import numpy as np
import matplotlib.pyplot as plt

# File path
file = "/content/DroneSound_noMuff.wav"

# Load audio
y, sr = librosa.load(file, sr=None, mono=False)

# Ensure y is 2D (channels x samples)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = y.shape[0]
print(f"Generating spectrograms for {num_channels} channels")

# Parameters for the spectrogram
n_fft = 2048       # FFT window size
hop_length = 512   # Number of samples between successive FFTs

# --- Loop through each channel and plot its spectrogram ---
for i in range(num_channels):
    # Calculate the power spectrogram for the current channel
    S = np.abs(librosa.stft(y[i], n_fft=n_fft, hop_length=hop_length))

    # Normalize the spectrogram to a 0-1 range
    S_normalized = S / np.max(S)

    # Plot the normalized spectrogram
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(S_normalized, sr=sr, hop_length=hop_length, x_axis='time', y_axis='hz', cmap='viridis', vmin=0, vmax=1)
    plt.colorbar(label='Amplitude (0-1)')
    plt.title(f'Normalized Spectrogram of Channel {i+1}')
    plt.tight_layout()
    plt.show()

"""Mean of all 5 channels

"""

import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

# File path
file = "/content/DroneSound_noMuff.wav"

# Load audio
y, sr = librosa.load(file, sr=None, mono=False)

# Ensure y is 2D (channels x samples)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = y.shape[0]
print(f"Loaded audio with {num_channels} channels.")

# Parameters for the spectrogram
n_fft = 2048
hop_length = 512

# List to store spectrograms
all_spectrograms = []

# Compute spectrogram for each channel
for i in range(num_channels):
    S = np.abs(librosa.stft(y[i], n_fft=n_fft, hop_length=hop_length))
    all_spectrograms.append(S)

# Stack and compute mean across channels
spectrogram_stack = np.stack(all_spectrograms, axis=-1)
mean_spectrogram = np.mean(spectrogram_stack, axis=-1)

# Normalize for plotting
mean_spectrogram_norm = mean_spectrogram / np.max(mean_spectrogram) if np.max(mean_spectrogram) > 0 else mean_spectrogram

# Plot mean spectrogram
plt.figure(figsize=(10, 5))
librosa.display.specshow(mean_spectrogram_norm, sr=sr, hop_length=hop_length,
                         x_axis='time', y_axis='hz', cmap='viridis', vmin=0, vmax=1)
plt.colorbar(label='Mean Amplitude (0-1)')
plt.title('Mean Spectrogram Across All Channels')
plt.tight_layout()
plt.show()

import librosa
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import stft

# File path
file = "/content/DroneSound_noMuff.wav"

# Load audio
y, sr = librosa.load(file, sr=None, mono=False)

# Ensure y is 2D (channels x samples)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = y.shape[0]
print(f"Loaded audio with {num_channels} channels.")

# STFT parameters
n_fft = 1024
hop_length = 512

# List to store coherence spectrograms for all channel pairs
coh_specs = []

# Compute STFT for each channel
stfts = [stft(y[ch], nperseg=n_fft, noverlap=n_fft - hop_length)[2] for ch in range(num_channels)]

# Compute magnitude-squared coherence for each channel pair
for i in range(num_channels):
    for j in range(i+1, num_channels):
        # Cross-spectral density
        Sxy = stfts[i] * np.conj(stfts[j])
        # Auto-spectral densities
        Sxx = np.abs(stfts[i])**2
        Syy = np.abs(stfts[j])**2
        # Coherence
        Cxy = np.abs(Sxy)**2 / (Sxx * Syy + 1e-10)  # add small epsilon to avoid division by zero
        coh_specs.append(Cxy)

# Stack and compute median across all channel pairs
coh_stack = np.stack(coh_specs, axis=-1)
median_coherence_spec = np.median(coh_stack, axis=-1)

# Plot median coherence spectrogram
plt.figure(figsize=(10, 5))
plt.pcolormesh(np.arange(median_coherence_spec.shape[1]) * hop_length / sr,
               np.linspace(0, sr/2, median_coherence_spec.shape[0]),
               median_coherence_spec,
               shading='gouraud', cmap='viridis', vmin=0, vmax=1)
plt.colorbar(label='Median Coherence (0-1)')
plt.xlabel('Time [s]')
plt.ylabel('Frequency [Hz]')
plt.title('Median Coherence Spectrogram Across All Channel Pairs')
plt.tight_layout()
plt.show()

import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

# File path
file = "/content/DroneSound_noMuff.wav"

# Load audio
y, sr = librosa.load(file, sr=None, mono=False)

# Ensure y is 2D (channels x samples)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = y.shape[0]
print(f"Generating spectrograms for {num_channels} channels")

# Parameters for the spectrogram
n_fft = 2048
hop_length = 512

# List to store all channel spectrograms
all_spectrograms = []

# Compute spectrogram for each channel
for i in range(num_channels):
    S = np.abs(librosa.stft(y[i], n_fft=n_fft, hop_length=hop_length))
    all_spectrograms.append(S)

# Stack and compute median across channels
spectrogram_stack = np.stack(all_spectrograms, axis=-1)  # shape: (freq_bins, time_frames, channels)
median_spectrogram = np.median(spectrogram_stack, axis=-1)  # median along the channel axis

# Convert to dB for plotting
median_spectrogram_db = librosa.amplitude_to_db(median_spectrogram, ref=np.max)

# Plot the median spectrogram
plt.figure(figsize=(10, 5))
librosa.display.specshow(median_spectrogram_db, sr=sr, hop_length=hop_length, x_axis='time', y_axis='hz', cmap='viridis')
plt.colorbar(label='Amplitude (dB)')
plt.title('Median Spectrogram Across All Channels')
plt.tight_layout()
plt.show()

import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

# File path
file = "/content/DroneSound_noMuff.wav"

# Load audio
y, sr = librosa.load(file, sr=None, mono=False)

# Ensure y is 2D (channels x samples)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = y.shape[0]
print(f"Generating spectrograms for {num_channels} channels")

# Parameters for the spectrogram
n_fft = 2048
hop_length = 512

# List to store all channel spectrograms
all_spectrograms = []

# Compute spectrogram for each channel
for i in range(num_channels):
    S = np.abs(librosa.stft(y[i], n_fft=n_fft, hop_length=hop_length))
    all_spectrograms.append(S)

# Stack and compute median across channels
spectrogram_stack = np.stack(all_spectrograms, axis=-1)  # shape: (freq_bins, time_frames, channels)
median_spectrogram = np.median(spectrogram_stack, axis=-1)  # median along the channel axis

# Normalize to 0-1 range
median_spectrogram_norm = median_spectrogram / np.max(median_spectrogram) if np.max(median_spectrogram) > 0 else median_spectrogram

# Plot the median spectrogram
plt.figure(figsize=(10, 5))
librosa.display.specshow(median_spectrogram_norm, sr=sr, hop_length=hop_length,
                         x_axis='time', y_axis='hz', cmap='viridis', vmin=0, vmax=1)
plt.colorbar(label='Normalized Amplitude (0-1)')
plt.title('Median Spectrogram Across All Channels (Normalized 0-1)')
plt.tight_layout()
plt.show()

import librosa
import librosa.display
import numpy as np
from scipy.signal import coherence, butter, filtfilt
import matplotlib.pyplot as plt

# --- Bandpass filter ---
def bandpass_filter(signal, sr, lowcut=300, highcut=3000, order=4):
    nyq = 0.5 * sr
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, signal)

# --- Load audio ---
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = min(5, y.shape[0])
print(f"Processing {num_channels} channels")

# --- Spectrogram parameters ---
n_fft = 2048
hop_length = 512

# Compute individual channel spectrograms
channel_specs = []
for i in range(num_channels):
    S = np.abs(librosa.stft(y[i], n_fft=n_fft, hop_length=hop_length))
    channel_specs.append(S)
channel_specs = np.stack(channel_specs, axis=-1)  # freq x time x channels

# --- Median coherence spectrogram ---
# Use consistent STFT parameters for coherence
nperseg_coh = n_fft
noverlap_coh = n_fft - hop_length


all_coh_specs = []
for i in range(num_channels):
    sig1 = bandpass_filter(y[i], sr)
    for j in range(i+1, num_channels):
        sig2 = bandpass_filter(y[j], sr)

        # Compute coherence
        f, _, Cxy = coherence(sig1, sig2, fs=sr, nperseg=nperseg_coh, noverlap=noverlap_coh)
        all_coh_specs.append(Cxy)

# Stack and compute median across all channel pairs
# Ensure all coherence specs have the same time dimension
min_time_bins = min(spec.shape[1] for spec in all_coh_specs)
all_coh_specs_trunc = [spec[:, :min_time_bins] for spec in all_coh_specs]

median_coh_spec = np.median(np.stack(all_coh_specs_trunc, axis=-1), axis=-1)

# --- Threshold at 0.6 ---
threshold_mask = np.where(median_coh_spec >= 0.6, 1, 0)

# --- Multiply each channel spectrogram with threshold mask ---
filtered_specs = []
for ch in range(num_channels):
    # Truncate spectrogram to match coherence time bins
    spec_trunc = channel_specs[:, :min_time_bins, ch]
    filtered_spec = spec_trunc * threshold_mask
    filtered_specs.append(filtered_spec)

# --- Plot results ---
for ch in range(num_channels):
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(filtered_specs[ch], sr=sr, hop_length=hop_length,
                             x_axis='time', y_axis='hz', cmap='viridis')
    plt.colorbar(label='Amplitude (0-1)')
    plt.title(f'Channel {ch+1} Spectrogram Filtered by Median Coherence ≥ 0.6')
    plt.tight_layout()
    plt.show()

import librosa
import librosa.display
import numpy as np
from scipy.signal import coherence, butter, filtfilt
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt

# --- Bandpass filter ---
def bandpass_filter(signal, sr, lowcut=300, highcut=3000, order=4):
    nyq = 0.5 * sr
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, signal)

# --- Load audio ---
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = min(5, y.shape[0])
print(f"Processing {num_channels} channels")

# --- STFT parameters ---
n_fft = 2048
hop_length = 512

# Compute individual channel spectrograms
channel_specs = []
for i in range(num_channels):
    S = np.abs(librosa.stft(y[i], n_fft=n_fft, hop_length=hop_length))
    channel_specs.append(S)
channel_specs = np.stack(channel_specs, axis=-1)  # freq x time x channels

# --- Median coherence spectrogram ---
window_size = 2048
step_size = 512
nperseg = 256

all_coh_specs = []
for i in range(num_channels):
    sig1 = bandpass_filter(y[i], sr)
    for j in range(i+1, num_channels):
        sig2 = bandpass_filter(y[j], sr)

        n_windows = (len(sig1) - window_size) // step_size + 1
        coh_spec = []
        for w in range(n_windows):
            start = w * step_size
            end = start + window_size
            f, Cxy = coherence(sig1[start:end], sig2[start:end], fs=sr, nperseg=nperseg)
            coh_spec.append(Cxy)
        coh_spec = np.array(coh_spec).T  # freq x time
        all_coh_specs.append(coh_spec)

# Truncate all coherence specs to same time bins
min_time_bins = min(spec.shape[1] for spec in all_coh_specs)
all_coh_specs_trunc = [spec[:, :min_time_bins] for spec in all_coh_specs]

# Median coherence
median_coh_spec = np.median(np.stack(all_coh_specs_trunc, axis=-1), axis=-1)  # freq x time

# --- Interpolate coherence along frequency to match STFT ---
freq_stft = np.linspace(0, sr/2, channel_specs.shape[0])
freq_coh = f
interp_func = interp1d(freq_coh, median_coh_spec, axis=0, kind='nearest', fill_value="extrapolate")
median_coh_interp = interp_func(freq_stft)  # freq x time, now matches STFT freq bins

# --- Threshold at 0.6 ---
threshold_mask = np.where(median_coh_interp >= 0.4, 1, 0)

# --- Multiply each channel spectrogram with threshold mask ---
filtered_specs = []
for ch in range(num_channels):
    spec_trunc = channel_specs[:, :threshold_mask.shape[1], ch]
    filtered_spec = spec_trunc * threshold_mask
    filtered_specs.append(filtered_spec)

# --- Plot filtered spectrograms ---
for ch in range(num_channels):
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(filtered_specs[ch], sr=sr, hop_length=hop_length,
                             x_axis='time', y_axis='hz', cmap='viridis', vmin=0, vmax=1)
    plt.colorbar(label='Amplitude (0-1)')
    plt.title(f'Channel {ch+1} Spectrogram Filtered by Median Coherence ≥ 0.4')
    plt.tight_layout()
    plt.show()

import librosa
import librosa.display
import numpy as np
import soundfile as sf  # You'll need this library to save the audio
from scipy.signal import coherence, butter, filtfilt
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt

# --- Bandpass filter ---
def bandpass_filter(signal, sr, lowcut=300, highcut=3000, order=4):
    nyq = 0.5 * sr
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, signal)

# --- Load audio ---
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = min(5, y.shape[0])
print(f"Processing {num_channels} channels")

# --- STFT parameters ---
n_fft = 2048
hop_length = 512

# --- Compute STFT for a single channel (e.g., channel 1) ---
# We save the full complex-valued spectrogram, S_full
S_full = librosa.stft(y[0], n_fft=n_fft, hop_length=hop_length)

# This is the magnitude you were plotting (for reference)
S_magnitude = np.abs(S_full)

# --- Compute the median coherence spectrogram and mask (as in your code) ---
# This part of your code remains the same to create the mask.
all_coh_specs = []
for i in range(num_channels):
    sig1 = bandpass_filter(y[i], sr)
    for j in range(i+1, num_channels):
        sig2 = bandpass_filter(y[j], sr)
        n_windows = (len(sig1) - 2048) // 512 + 1
        coh_spec = []
        for w in range(n_windows):
            start = w * 512
            end = start + 2048
            f, Cxy = coherence(sig1[start:end], sig2[start:end], fs=sr, nperseg=256)
            coh_spec.append(Cxy)
        coh_spec = np.array(coh_spec).T
        all_coh_specs.append(coh_spec)
min_time_bins = min(spec.shape[1] for spec in all_coh_specs)
all_coh_specs_trunc = [spec[:, :min_time_bins] for spec in all_coh_specs]
median_coh_spec = np.median(np.stack(all_coh_specs_trunc, axis=-1), axis=-1)
freq_stft = np.linspace(0, sr/2, S_full.shape[0])
freq_coh = f
interp_func = interp1d(freq_coh, median_coh_spec, axis=0, kind='nearest', fill_value="extrapolate")
median_coh_interp = interp_func(freq_stft)
threshold_mask = np.where(median_coh_interp >= 0.6, 1, 0)
threshold_mask = threshold_mask[:, :S_full.shape[1]] # Truncate mask to match STFT shape

# --- Apply the mask to the full complex spectrogram ---
S_full_filtered = S_full * threshold_mask

# --- Plot the filtered magnitude spectrogram for verification ---
plt.figure(figsize=(10, 4))
librosa.display.specshow(np.abs(S_full_filtered), sr=sr, hop_length=hop_length,
                         x_axis='time', y_axis='hz', cmap='viridis', vmin=0, vmax=1)
plt.colorbar(label='Filtered Amplitude (0-1)')
plt.title('Channel 1 Spectrogram Filtered by Median Coherence ≥ 0.6')
plt.tight_layout()
plt.show()

import librosa
import librosa.display
import numpy as np
from scipy.signal import coherence, butter, filtfilt
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt

# --- Bandpass filter ---
def bandpass_filter(signal, sr, lowcut=300, highcut=3000, order=4):
    nyq = 0.5 * sr
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, signal)

# --- Load audio ---
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = min(5, y.shape[0])
print(f"Processing {num_channels} channels")

# --- STFT parameters ---
n_fft = 2048
hop_length = 512

# Compute individual channel spectrograms (complex STFT for ISTFT)
channel_specs_complex = []
for i in range(num_channels):
    S = librosa.stft(y[i], n_fft=n_fft, hop_length=hop_length)
    channel_specs_complex.append(S)
channel_specs_complex = np.stack(channel_specs_complex, axis=-1)  # freq x time x channels

# Also compute magnitude for filtering
channel_specs_mag = np.abs(channel_specs_complex)

# --- Median coherence spectrogram ---
window_size = 2048
step_size = 512
nperseg = 256

all_coh_specs = []
for i in range(num_channels):
    sig1 = bandpass_filter(y[i], sr)
    for j in range(i+1, num_channels):
        sig2 = bandpass_filter(y[j], sr)

        n_windows = (len(sig1) - window_size) // step_size + 1
        coh_spec = []
        for w in range(n_windows):
            start = w * step_size
            end = start + window_size
            f, Cxy = coherence(sig1[start:end], sig2[start:end], fs=sr, nperseg=nperseg)
            coh_spec.append(Cxy)
        coh_spec = np.array(coh_spec).T  # freq x time
        all_coh_specs.append(coh_spec)

# Truncate all coherence specs to same time bins
min_time_bins = min(spec.shape[1] for spec in all_coh_specs)
all_coh_specs_trunc = [spec[:, :min_time_bins] for spec in all_coh_specs]

# Median coherence
median_coh_spec = np.median(np.stack(all_coh_specs_trunc, axis=-1), axis=-1)  # freq x time

# --- Interpolate coherence along frequency to match STFT ---
freq_stft = np.linspace(0, sr/2, channel_specs_mag.shape[0])
freq_coh = f
interp_func = interp1d(freq_coh, median_coh_spec, axis=0, kind='nearest', fill_value="extrapolate")
median_coh_interp = interp_func(freq_stft)  # freq x time

# --- Threshold at 0.6 ---
threshold_mask = np.where(median_coh_interp >= 0.6, 1, 0)

# --- Apply mask to each channel (magnitude) ---
filtered_specs_complex = []
for ch in range(num_channels):
    spec = channel_specs_complex[:, :threshold_mask.shape[1], ch]
    filtered_spec = spec * threshold_mask  # keep original phase
    filtered_specs_complex.append(filtered_spec)

# --- ISTFT: Convert filtered spectrograms back to time-domain signals ---
reconstructed_signals = []
for ch in range(num_channels):
    y_rec = librosa.istft(filtered_specs_complex[ch], hop_length=hop_length)
    reconstructed_signals.append(y_rec)

# --- Plot filtered spectrograms and reconstructed waveforms ---
for ch in range(num_channels):
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(np.abs(filtered_specs_complex[ch]), sr=sr, hop_length=hop_length,
                             x_axis='time', y_axis='hz', cmap='viridis', vmin=0, vmax=1)
    plt.colorbar(label='Amplitude (0-1)')
    plt.title(f'Channel {ch+1} Filtered Spectrogram')
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(10, 2))
    plt.plot(reconstructed_signals[ch])
    plt.title(f'Channel {ch+1} Reconstructed Time-Domain Signal')
    plt.xlabel('Samples')
    plt.ylabel('Amplitude')
    plt.tight_layout()
    plt.show()

import librosa
import librosa.display
import numpy as np
import soundfile as sf
from scipy.signal import coherence, butter, filtfilt
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt

# --- Bandpass filter function ---
def bandpass_filter(signal, sr, lowcut=300, highcut=3000, order=4):
    nyq = 0.5 * sr
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, signal)

# --- Load audio file ---
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = min(5, y.shape[0])
print(f"Processing {num_channels} channels from input audio.")

# --- STFT parameters ---
n_fft = 2048
hop_length = 512

# --- Compute STFT for a single channel (channel 1) ---
S_full = librosa.stft(y[0], n_fft=n_fft, hop_length=hop_length)

# --- Compute the median coherence spectrogram and mask ---
window_size = 2048
step_size = 512
nperseg = 256

all_coh_specs = []
for i in range(num_channels):
    sig1 = bandpass_filter(y[i], sr)
    for j in range(i+1, num_channels):
        sig2 = bandpass_filter(y[j], sr)

        n_windows = (len(sig1) - window_size) // step_size + 1
        coh_spec = []
        for w in range(n_windows):
            start = w * step_size
            end = start + window_size
            f, Cxy = coherence(sig1[start:end], sig2[start:end], fs=sr, nperseg=nperseg)
            coh_spec.append(Cxy)
        coh_spec = np.array(coh_spec).T
        all_coh_specs.append(coh_spec)

min_time_bins = min(spec.shape[1] for spec in all_coh_specs)
all_coh_specs_trunc = [spec[:, :min_time_bins] for spec in all_coh_specs]
median_coh_spec = np.median(np.stack(all_coh_specs_trunc, axis=-1), axis=-1)
freq_stft = np.linspace(0, sr/2, S_full.shape[0])
freq_coh = f
interp_func = interp1d(freq_coh, median_coh_spec, axis=0, kind='nearest', fill_value="extrapolate")
median_coh_interp = interp_func(freq_stft)
threshold_mask = np.where(median_coh_interp >= 0.6, 1, 0)

# --- CORRECTED: Ensure both arrays have the same time dimension ---
# Find the minimum time dimension between S_full and threshold_mask
min_time_dim = min(S_full.shape[1], threshold_mask.shape[1])

# Truncate both arrays to this minimum time dimension
S_full_trunc = S_full[:, :min_time_dim]
threshold_mask_trunc = threshold_mask[:, :min_time_dim]

print(f"Original S_full shape: {S_full.shape}")
print(f"Original threshold_mask shape: {threshold_mask.shape}")
print(f"Truncated S_full shape: {S_full_trunc.shape}")
print(f"Truncated threshold_mask shape: {threshold_mask_trunc.shape}")


# --- Apply the mask to the full complex spectrogram ---
S_full_filtered = S_full_trunc * threshold_mask_trunc

# --- Plot the filtered magnitude spectrogram for verification ---
plt.figure(figsize=(10, 4))
librosa.display.specshow(np.abs(S_full_filtered), sr=sr, hop_length=hop_length,
                         x_axis='time', y_axis='hz', cmap='viridis', vmin=0, vmax=1)
plt.colorbar(label='Filtered Amplitude (0-1)')
plt.title('Filtered Spectrogram of Channel 1')
plt.tight_layout()
plt.show()

# --- Perform the Inverse Short-Time Fourier Transform (ISTFT) and save ---
y_reconstructed = librosa.istft(S_full_filtered, hop_length=hop_length)
output_file = "/content/reconstructed_audio.wav"
sf.write(output_file, y_reconstructed, sr)

print(f"\nReconstructed audio saved to '{output_file}'")

import librosa
import librosa.display
import numpy as np
import soundfile as sf
from scipy.signal import coherence, butter, filtfilt
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt

# --- Bandpass filter function ---
def bandpass_filter(signal, sr, lowcut=300, highcut=3000, order=4):
    nyq = 0.5 * sr
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, signal)

# --- Load audio file ---
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = min(5, y.shape[0])
print(f"Processing {num_channels} channels from input audio.")

# --- STFT parameters ---
n_fft = 2048
hop_length = 512

# --- Compute STFT for all channels ---
channel_specs_complex = []
for ch in range(num_channels):
    S = librosa.stft(y[ch], n_fft=n_fft, hop_length=hop_length)
    channel_specs_complex.append(S)
channel_specs_complex = np.stack(channel_specs_complex, axis=-1)  # freq x time x channels

# --- Compute median coherence spectrogram ---
window_size = 2048
step_size = 512
nperseg = 256

all_coh_specs = []
for i in range(num_channels):
    sig1 = bandpass_filter(y[i], sr)
    for j in range(i+1, num_channels):
        sig2 = bandpass_filter(y[j], sr)
        n_windows = (len(sig1) - window_size) // step_size + 1
        coh_spec = []
        for w in range(n_windows):
            start = w * step_size
            end = start + window_size
            f, Cxy = coherence(sig1[start:end], sig2[start:end], fs=sr, nperseg=nperseg)
            coh_spec.append(Cxy)
        coh_spec = np.array(coh_spec).T
        all_coh_specs.append(coh_spec)

# Truncate all coherence specs to same time bins
min_time_bins = min(spec.shape[1] for spec in all_coh_specs)
all_coh_specs_trunc = [spec[:, :min_time_bins] for spec in all_coh_specs]

# Median coherence
median_coh_spec = np.median(np.stack(all_coh_specs_trunc, axis=-1), axis=-1)

# Interpolate along frequency to match STFT
freq_stft = np.linspace(0, sr/2, channel_specs_complex.shape[0])
freq_coh = f
interp_func = interp1d(freq_coh, median_coh_spec, axis=0, kind='nearest', fill_value="extrapolate")
median_coh_interp = interp_func(freq_stft)

# Threshold at 0.3
threshold_mask = np.where(median_coh_interp >= 0.3, 1, 0)

# --- Apply mask and ISTFT for all channels ---
reconstructed_signals = []
for ch in range(num_channels):
    S_full = channel_specs_complex[:, :, ch]

    # Ensure both arrays have the same time dimension
    min_time_dim = min(S_full.shape[1], threshold_mask.shape[1])
    S_full_trunc = S_full[:, :min_time_dim]
    threshold_mask_trunc = threshold_mask[:, :min_time_dim]

    # Apply mask
    S_filtered = S_full_trunc * threshold_mask_trunc

    # ISTFT
    y_rec = librosa.istft(S_filtered, hop_length=hop_length)
    reconstructed_signals.append(y_rec)

    # Save reconstructed audio
    output_file = f"/content/reconstructed_channel_{ch+1}.wav"
    sf.write(output_file, y_rec, sr)
    print(f"Channel {ch+1} reconstructed audio saved to '{output_file}'")

    # Optional: plot filtered spectrogram
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(np.abs(S_filtered), sr=sr, hop_length=hop_length,
                             x_axis='time', y_axis='hz', cmap='viridis', vmin=0, vmax=1)
    plt.colorbar(label='Filtered Amplitude (0-1)')
    plt.title(f'Filtered Spectrogram of Channel {ch+1}')
    plt.tight_layout()
    plt.show()



import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from typing import Optional, Tuple

def create_spectrogram(
    audio_data: np.ndarray,
    sample_rate: int,
    n_fft: int = 2048,
    hop_length: int = 512,
    window: str = 'hann',
    cmap: str = 'viridis',
    figsize: Tuple[int, int] = (12, 5),
    title: Optional[str] = None,
    logging: bool = False,
    yscale: str = 'log',
):
    """
    Create and plot a spectrogram for the given audio data.
    """
    # Compute STFT
    S_complex = librosa.stft(audio_data, n_fft=n_fft, hop_length=hop_length, window=window)
    S_mag = np.abs(S_complex)

    # Convert to dB if logging is True
    S_plot = librosa.amplitude_to_db(S_mag, ref=np.max) if logging else S_mag

    # Frequency and time axes
    freqs = librosa.fft_frequencies(sr=sample_rate, n_fft=n_fft)
    times = librosa.frames_to_time(np.arange(S_mag.shape[1]), sr=sample_rate, hop_length=hop_length)

    # Plot
    plt.figure(figsize=figsize)
    librosa.display.specshow(S_plot, sr=sample_rate, hop_length=hop_length,
                             x_axis='time', y_axis='log' if yscale=='log' else 'linear',
                             cmap=cmap)
    plt.colorbar(format='%+2.0f dB' if logging else '%0.2f', label='Amplitude' if not logging else 'dB')
    if title:
        plt.title(title)
    plt.tight_layout()
    plt.show()

    return S_mag, freqs, times

# --- Example usage ---
# Load your audio
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)

# Handle multi-channel audio (plot channel 1)
if y.ndim > 1:
    y_channel = y[0]  # first channel
else:
    y_channel = y

# Create spectrogram
S, freqs, times = create_spectrogram(
    audio_data=y_channel,
    sample_rate=sr,
    n_fft=2048,
    hop_length=512,
    logging=True,
    title="Spectrogram - Channel 1"
)

import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from typing import Optional, Tuple

def create_spectrogram(
    audio_data: np.ndarray,
    sample_rate: int,
    n_fft: int = 2048,
    hop_length: int = 512,
    window: str = 'hann',
    cmap: str = 'viridis',
    figsize: Tuple[int, int] = (12, 5),
    title: Optional[str] = None,
    logging: bool = False,
    yscale: str = 'log',
):
    """
    Create and plot a spectrogram for the given audio data.
    """
    # Compute STFT
    S_complex = librosa.stft(audio_data, n_fft=n_fft, hop_length=hop_length, window=window)
    S_mag = np.abs(S_complex)

    # Convert to dB if logging is True
    S_plot = librosa.amplitude_to_db(S_mag, ref=np.max) if logging else S_mag

    # Frequency and time axes
    freqs = librosa.fft_frequencies(sr=sample_rate, n_fft=n_fft)
    times = librosa.frames_to_time(np.arange(S_mag.shape[1]), sr=sample_rate, hop_length=hop_length)

    # Plot
    plt.figure(figsize=figsize)
    librosa.display.specshow(S_plot, sr=sample_rate, hop_length=hop_length,
                             x_axis='time', y_axis='log' if yscale=='log' else 'linear',
                             cmap=cmap)
    plt.colorbar(format='%+2.0f dB' if logging else '%0.2f', label='Amplitude' if not logging else 'dB')
    if title:
        plt.title(title)
    plt.tight_layout()
    plt.show()

    return S_mag, freqs, times

# --- Load multi-channel audio ---
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)

# --- Loop over all channels and plot spectrograms ---
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = y.shape[0]
print(f"Number of channels: {num_channels}")

for ch in range(num_channels):
    print(f"Creating spectrogram for Channel {ch+1}")
    create_spectrogram(
        audio_data=y[ch],
        sample_rate=sr,
        n_fft=2048,
        hop_length=512,
        logging=True,
        title=f"Spectrogram - Channel {ch+1}"
    )

import librosa
import librosa.display
import numpy as np
import soundfile as sf
from scipy.signal import coherence, butter, filtfilt
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt

# --- Bandpass filter function ---
def bandpass_filter(signal, sr, lowcut=300, highcut=3000, order=4):
    nyq = 0.5 * sr
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, signal)

# --- Load audio file ---
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = min(5, y.shape[0])
print(f"Processing {num_channels} channels from input audio.")

# --- STFT parameters ---
n_fft = 2048
hop_length = 512

# --- Compute STFT for a single channel (channel 1) ---
S_full = librosa.stft(y[0], n_fft=n_fft, hop_length=hop_length)

# --- Compute the median coherence spectrogram and mask ---
window_size = 2048
step_size = 512
nperseg = 256

all_coh_specs = []
for i in range(num_channels):
    sig1 = bandpass_filter(y[i], sr)
    for j in range(i+1, num_channels):
        sig2 = bandpass_filter(y[j], sr)

        n_windows = (len(sig1) - window_size) // step_size + 1
        coh_spec = []
        for w in range(n_windows):
            start = w * step_size
            end = start + window_size
            f, Cxy = coherence(sig1[start:end], sig2[start:end], fs=sr, nperseg=nperseg)
            coh_spec.append(Cxy)
        coh_spec = np.array(coh_spec).T
        all_coh_specs.append(coh_spec)

min_time_bins = min(spec.shape[1] for spec in all_coh_specs)
all_coh_specs_trunc = [spec[:, :min_time_bins] for spec in all_coh_specs]
median_coh_spec = np.median(np.stack(all_coh_specs_trunc, axis=-1), axis=-1)
freq_stft = np.linspace(0, sr/2, S_full.shape[0])
freq_coh = f
interp_func = interp1d(freq_coh, median_coh_spec, axis=0, kind='nearest', fill_value="extrapolate")
median_coh_interp = interp_func(freq_stft)

# --- CORRECTED LINE ---
# Change the comma to a period for the number 0.3
threshold_mask = np.where(median_coh_interp >= 0.3, 1, 0)

# --- Apply the mask to the full complex spectrogram ---
# Find the minimum time dimension between S_full and threshold_mask
min_time_dim = min(S_full.shape[1], threshold_mask.shape[1])
S_full_trunc = S_full[:, :min_time_dim]
threshold_mask_trunc = threshold_mask[:, :min_time_dim]

S_full_filtered = S_full_trunc * threshold_mask_trunc

# --- Plot the filtered magnitude spectrogram for verification ---
plt.figure(figsize=(10, 4))
librosa.display.specshow(np.abs(S_full_filtered), sr=sr, hop_length=hop_length,
                         x_axis='time', y_axis='hz', cmap='viridis', vmin=0, vmax=1)
plt.colorbar(label='Filtered Amplitude (0-1)')
plt.title('Filtered Spectrogram of Channel 1')
plt.tight_layout()
plt.show()

# --- Perform the Inverse Short-Time Fourier Transform (ISTFT) and save ---
y_reconstructed = librosa.istft(S_full_filtered, hop_length=hop_length)
output_file = "/content/reconstructed_audio.wav"
sf.write(output_file, y_reconstructed, sr)

print(f"\nReconstructed audio saved to '{output_file}'")

import librosa
import librosa.display
import numpy as np
import soundfile as sf
from scipy.signal import coherence, butter, filtfilt
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt
import os

# --- Bandpass filter function ---
def bandpass_filter(signal, sr, lowcut=300, highcut=3000, order=4):
    nyq = 0.5 * sr
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, signal)

# --- Load audio file ---
file = "/content/esp32-1-20250922-183948-962780295.wav"
y, sr = librosa.load(file, sr=None, mono=False)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = min(5, y.shape[0])
print(f"Processing {num_channels} channels from input audio.")

# --- STFT parameters ---
n_fft = 2048
hop_length = 512

# --- Compute STFT for each channel ---
channel_specs = []
for i in range(num_channels):
    S = librosa.stft(y[i], n_fft=n_fft, hop_length=hop_length)
    channel_specs.append(S)
channel_specs = np.stack(channel_specs, axis=-1)  # freq x time x channels

# --- Coherence computation ---
window_size = 2048
step_size = 512
nperseg = 256

all_coh_specs = []
for i in range(num_channels):
    sig1 = bandpass_filter(y[i], sr)
    for j in range(i+1, num_channels):
        sig2 = bandpass_filter(y[j], sr)

        n_windows = (len(sig1) - window_size) // step_size + 1
        coh_spec = []
        for w in range(n_windows):
            start = w * step_size
            end = start + window_size
            f, Cxy = coherence(sig1[start:end], sig2[start:end], fs=sr, nperseg=nperseg)
            coh_spec.append(Cxy)
        coh_spec = np.array(coh_spec).T
        all_coh_specs.append(coh_spec)

# Truncate coherence specs to same time bins
min_time_bins = min(spec.shape[1] for spec in all_coh_specs)
all_coh_specs_trunc = [spec[:, :min_time_bins] for spec in all_coh_specs]

# Median coherenceblob:https://colab.research.google.com/824c2e71-b075-4735-9c14-8b6c0a130b5b
median_coh_spec = np.median(np.stack(all_coh_specs_trunc, axis=-1), axis=-1)

# Interpolate coherence to STFT frequencies
freq_stft = np.linspace(0, sr/2, channel_specs.shape[0])
freq_coh = f
interp_func = interp1d(freq_coh, median_coh_spec, axis=0, kind='nearest', fill_value="extrapolate")
median_coh_interp = interp_func(freq_stft)

# Threshold mask
threshold_mask = np.where(median_coh_interp >= 0.3, 1, 0)

# --- Apply mask + reconstruct per channel ---
output_dir = "/content/results"
os.makedirs(output_dir, exist_ok=True)

reconstructed_signals = []
for ch in range(num_channels):
    # Match time dimension
    min_time_dim = min(channel_specs.shape[1], threshold_mask.shape[1])
    S_trunc = channel_specs[:, :min_time_dim, ch]
    mask_trunc = threshold_mask[:, :min_time_dim]

    # Apply mask
    S_filtered = S_trunc * mask_trunc

    # Save spectrogram image
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(np.abs(S_filtered), sr=sr, hop_length=hop_length,
                             x_axis='time', y_axis='hz', cmap='viridis', vmin=0, vmax=1)
    plt.colorbar(label='Filtered Amplitude (0-1)')
    plt.title(f'Filtered Spectrogram - Channel {ch+1}')
    plt.tight_layout()

    image_file = os.path.join(output_dir, f"filtered_spectrogram_channel{ch+1}.png")
    plt.savefig(image_file, dpi=300)
    plt.close()
    print(f"Saved spectrogram image1: {image_file}")

    # Reconstruct audio
    y_rec = librosa.istft(S_filtered, hop_length=hop_length)
    reconstructed_signals.append(y_rec)

    # Save audio
    audio_file = os.path.join(output_dir, f"reconstructed_channel{ch+1}.wav")
    sf.write(audio_file, y_rec, sr)
    print(f"Saved reconstructed audio: {audio_file}")

#Only spectra combined
import librosa
import numpy as np
import soundfile as sf
import matplotlib.pyplot as plt
import os

# --- Load audio file ---
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = min(5, y.shape[0])
print(f"Processing {num_channels} channels from input audio.")

# --- Output directory ---
output_dir = "/content/results_spectra"
os.makedirs(output_dir, exist_ok=True)

# --- Compute spectra for each channel ---
spectra = []
freqs = None
for ch in range(num_channels):
    Y = np.fft.rfft(y[ch])               # one-sided FFT
    mag = np.abs(Y) / len(Y)             # magnitude spectrum
    freqs = np.fft.rfftfreq(len(y[ch]), 1/sr)
    spectra.append(mag)

spectra = np.array(spectra)              # shape: channels x freqs

# --- Combine spectra (e.g., average across channels) ---
combined_spectrum = np.mean(spectra, axis=0)

# --- Plot and save spectra ---
plt.figure(figsize=(10, 6))

# Plot each channel
for ch in range(num_channels):
    plt.semilogy(freqs, spectra[ch], label=f"Channel {ch+1}", alpha=0.6)

# Plot combined
plt.semilogy(freqs, combined_spectrum, 'k', linewidth=2, label="Combined (mean)")

plt.xlabel("Frequency (Hz)")
plt.ylabel("Magnitude (log scale)")
plt.title("Spectra of Channels and Combined Spectrum")
plt.legend()
plt.grid(True, which="both", linestyle="--", alpha=0.5)

image_file = os.path.join(output_dir, "combined_spectra.png")
plt.savefig(image_file, dpi=300)
plt.close()

print(f"Saved spectra plot: {image_file}")

#individual spectra

import librosa
import numpy as np
import matplotlib.pyplot as plt
import os

# --- Load audio file ---
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = min(5, y.shape[0])
print(f"Processing {num_channels} channels from input audio.")

# --- Output directory ---
output_dir = "/content/results_spectra_channels"
os.makedirs(output_dir, exist_ok=True)

# --- Compute and save spectra for each channel ---
for ch in range(num_channels):
    # FFT
    Y = np.fft.rfft(y[ch])
    mag = np.abs(Y) / len(Y)
    freqs = np.fft.rfftfreq(len(y[ch]), 1/sr)

    # Plot
    plt.figure(figsize=(10, 5))
    plt.semilogy(freqs, mag, color="blue")
    plt.xlabel("Frequency (Hz)")
    plt.ylabel("Magnitude (log scale)")
    plt.title(f"Spectrum - Channel {ch+1}")
    plt.grid(True, which="both", linestyle="--", alpha=0.5)

    # Save
    image_file = os.path.join(output_dir, f"spectrum_channel{ch+1}.png")
    plt.savefig(image_file, dpi=300)
    plt.close()

    print(f"Saved spectrum image: {image_file}")

"""Squaring Filter

"""

import librosa
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import coherence, butter, filtfilt
from scipy.interpolate import interp1d
import os

# --- Bandpass filter function ---
def bandpass_filter(signal, sr, lowcut=300, highcut=3000, order=4):
    nyq = 0.5 * sr
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, signal)

# --- Load audio ---
file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(file, sr=None, mono=False)
if y.ndim == 1:
    y = np.expand_dims(y, axis=0)

num_channels = min(5, y.shape[0])   # Use first 4 channels
print(f"Processing {num_channels} channels.")

# --- STFT params ---
n_fft = 2048
hop_length = 512

# --- Compute STFT for each channel ---
channel_specs = []
for i in range(num_channels):
    S = librosa.stft(y[i], n_fft=n_fft, hop_length=hop_length)
    channel_specs.append(S)
channel_specs = np.stack(channel_specs, axis=-1)

# --- Coherence computation ---
window_size = 2048
step_size = 512
nperseg = 256

all_coh_specs = []
for i in range(num_channels):
    sig1 = bandpass_filter(y[i], sr)
    for j in range(i+1, num_channels):
        sig2 = bandpass_filter(y[j], sr)

        n_windows = (len(sig1) - window_size) // step_size + 1
        coh_spec = []
        for w in range(n_windows):
            start = w * step_size
            end = start + window_size
            f, Cxy = coherence(sig1[start:end], sig2[start:end], fs=sr, nperseg=nperseg)
            coh_spec.append(Cxy)
        coh_spec = np.array(coh_spec).T
        all_coh_specs.append(coh_spec)

# --- Median coherence spectrum ---
min_time_bins = min(spec.shape[1] for spec in all_coh_specs)
all_coh_specs_trunc = [spec[:, :min_time_bins] for spec in all_coh_specs]
median_coh_spec = np.median(np.stack(all_coh_specs_trunc, axis=-1), axis=-1)

# --- Square as filter ---
coh_filter = median_coh_spec ** 2   # squaring enhances strong coherence, suppresses weak
coh_filter = np.clip(coh_filter, 0, 1)

# --- Interpolate to STFT frequencies ---
freq_stft = np.linspace(0, sr/2, channel_specs.shape[0])
interp_func = interp1d(f, coh_filter, axis=0, kind='nearest', fill_value="extrapolate")
coh_interp = interp_func(freq_stft)

# --- Apply filter ---
output_dir = "/content/results_squared_filter"
os.makedirs(output_dir, exist_ok=True)

for ch in range(num_channels):
    min_time_dim = min(channel_specs.shape[1], coh_interp.shape[1])
    S_trunc = channel_specs[:, :min_time_dim, ch]
    mask_trunc = coh_interp[:, :min_time_dim]

    # Apply mask
    S_filtered = S_trunc * mask_trunc

    # Plot comparison (original vs filtered)
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    librosa.display.specshow(librosa.amplitude_to_db(np.abs(S_trunc), ref=np.max),
                             sr=sr, hop_length=hop_length, x_axis='time', y_axis='hz')
    plt.colorbar(label="dB")
    plt.title(f"Original Spectrogram - Ch{ch+1}")

    plt.subplot(1, 2, 2)
    librosa.display.specshow(librosa.amplitude_to_db(np.abs(S_filtered), ref=np.max),
                             sr=sr, hop_length=hop_length, x_axis='time', y_axis='hz')
    plt.colorbar(label="dB")
    plt.title(f"Filtered Spectrogram (Squared Coherence) - Ch{ch+1}")

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"channel{ch+1}_comparison.png"), dpi=300)
    plt.close()

    print(f"Saved comparison spectrogram for Channel {ch+1}")

import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from typing import Tuple

# --- Function to convert 1D audio to TF map ---
def audio_to_tf_map(audio, n_fft=1024, hop_length=512):
    """
    Converts 1D audio signal to a TF map using STFT.
    Returns magnitude spectrogram (freq_bins x time_frames).
    """
    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)
    tf_map = np.abs(stft)
    return tf_map

# --- Corrected pixel-wise coherence function ---
def calc_coherence(tf1, tf2):
    """
    Pixel-wise coherence between two TF maps.
    Returns coherence of same shape as input TF maps.
    """
    tf1_norm = tf1 / (np.max(tf1, axis=0, keepdims=True) + 1e-8)
    tf2_norm = tf2 / (np.max(tf2, axis=0, keepdims=True) + 1e-8)
    coherence = tf1_norm * tf2_norm  # shape: (freq_bins, time_frames)
    coherence = coherence**2         # enhance strong coherence
    coherence = np.clip(coherence, 0, 1)
    return coherence

# --- Helper: map pair index to channel indices ---
def _pair_index_to_channels(pair_idx: int, K: int) -> Tuple[int, int]:
    i = 0
    remaining = pair_idx
    while remaining >= (K - 1 - i):
        remaining -= (K - 1 - i)
        i += 1
    j = i + 1 + remaining
    return i, j

# --- Main coherence processing function ---
def process_multi_channel_coherence(
        tf_maps: np.ndarray,
        calc_coherence: callable,
        coherence_threshold: float = 0.5,
        logging: bool = False
    ) -> Tuple[np.ndarray, np.ndarray]:
    K, L, M = tf_maps.shape
    if K < 2:
        raise ValueError("At least 2 channels are required for coherence analysis")

    num_pairs = K * (K - 1) // 2
    if logging:
        print(f"[COHERENCE] Processing {K} channels, {num_pairs} channel pairs")
        print(f"[COHERENCE] Time-frequency map shape: {L} freq bins × {M} time samples")

    coherence_streams = np.zeros((num_pairs, L, M))
    max_coherence = np.zeros((L, M))
    best_pair_indices = np.zeros((L, M), dtype=int)

    pair_idx = 0
    for i in range(K):
        for j in range(i + 1, K):
            if logging:
                print(f"[COHERENCE] Calculating coherence for channel pair ({i}, {j})")

            coherence = calc_coherence(tf_maps[i], tf_maps[j])
            coherence_streams[pair_idx] = coherence

            better_mask = coherence > max_coherence
            max_coherence[better_mask] = coherence[better_mask]
            best_pair_indices[better_mask] = pair_idx

            pair_idx += 1

    # Average TF map of the most coherent pair
    average_tf_map = np.zeros((L, M))
    for l in range(L):
        for m in range(M):
            best_pair = best_pair_indices[l, m]
            i, j = _pair_index_to_channels(best_pair, K)
            average_tf_map[l, m] = (tf_maps[i, l, m] + tf_maps[j, l, m]) / 2.0

    coherence_mask = max_coherence >= coherence_threshold

    if logging:
        coherence_stats = {
            'mean': np.mean(max_coherence),
            'std': np.std(max_coherence),
            'min': np.min(max_coherence),
            'max': np.max(max_coherence),
            'above_threshold': np.sum(coherence_mask) / (L * M) * 100
        }
        print(f"[COHERENCE] Coherence statistics:")
        print(f"  Mean: {coherence_stats['mean']:.3f}")
        print(f"  Std: {coherence_stats['std']:.3f}")
        print(f"  Range: [{coherence_stats['min']:.3f}, {coherence_stats['max']:.3f}]")
        print(f"  Above threshold ({coherence_threshold}): {coherence_stats['above_threshold']:.1f}%")

    return average_tf_map, coherence_mask

# --- Load multi-channel audio ---
audio_file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(audio_file, sr=None, mono=False)  # shape: (channels, samples)
print("Audio shape:", y.shape)

# --- Convert each channel to TF maps ---
K = y.shape[0]
tf_maps = np.array([audio_to_tf_map(y[ch]) for ch in range(K)])
print("TF maps shape:", tf_maps.shape)

# --- Compute coherence ---
average_tf_map, coherence_mask = process_multi_channel_coherence(
    tf_maps=tf_maps,
    calc_coherence=calc_coherence,
    coherence_threshold=0.5,
    logging=True
)

print("Average TF map shape:", average_tf_map.shape)
print("Coherence mask shape:", coherence_mask.shape)

# --- Visualization ---

# Plot Average TF Map
plt.figure(figsize=(12, 5))
librosa.display.specshow(
    20 * np.log10(average_tf_map + 1e-8),  # Convert magnitude to dB
    sr=sr,
    hop_length=512,
    y_axis='log',
    x_axis='time'
)
plt.colorbar(format='%+2.0f dB')
plt.title("Average TF Map of Most Coherent Pair")
plt.tight_layout()
plt.show()

# Plot Coherence Mask
plt.figure(figsize=(12, 5))
plt.imshow(coherence_mask, aspect='auto', origin='lower', cmap='viridis')
plt.colorbar(label='Coherence Mask')
plt.ylabel('Frequency Bin')
plt.xlabel('Time Frame')
plt.title("Coherence Mask (True=Coherent)")
plt.tight_layout()
plt.show()

# Optional: Overlay mask on TF map
plt.figure(figsize=(12, 5))
plt.imshow(20*np.log10(average_tf_map+1e-8), aspect='auto', origin='lower', cmap='magma')
plt.imshow(coherence_mask, aspect='auto', origin='lower', cmap='cool', alpha=0.4)
plt.colorbar(label='Magnitude (dB)')
plt.ylabel('Frequency Bin')
plt.xlabel('Time Frame')
plt.title("Average TF Map with Coherence Mask Overlay")
plt.tight_layout()
plt.show()

#seperate coherence
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from typing import Tuple

# --- Convert 1D audio to TF map ---
def audio_to_tf_map(audio, n_fft=1024, hop_length=512):
    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)
    tf_map = np.abs(stft)
    return tf_map

# --- Pixel-wise coherence ---
def calc_coherence(tf1, tf2):
    tf1_norm = tf1 / (np.max(tf1, axis=0, keepdims=True) + 1e-8)
    tf2_norm = tf2 / (np.max(tf2, axis=0, keepdims=True) + 1e-8)
    coherence = (tf1_norm * tf2_norm)**2
    return np.clip(coherence, 0, 1)

# --- Process coherence for each channel separately ---
def per_channel_coherence(tf_maps: np.ndarray, coherence_threshold=0.5, logging=False):
    """
    For each channel, compute pixel-wise coherence with all other channels.
    Returns a dictionary: channel index -> average coherence map & mask
    """
    K, L, M = tf_maps.shape
    results = {}

    for ch in range(K):
        if logging:
            print(f"\n[CHANNEL {ch}] Processing coherence with other channels")
        # Initialize max coherence map for this channel
        max_coherence = np.zeros((L, M))
        best_pair_map = np.zeros((L, M), dtype=int)


        # Average TF map for this channel with its most coherent "partner" per pixel
        avg_tf_map = np.zeros((L, M))
        for l in range(L):
            for m in range(M):
                partner_ch = best_pair_map[l, m]
                avg_tf_map[l, m] = (tf_maps[ch, l, m] + tf_maps[partner_ch, l, m]) / 2.0

        coherence_mask = max_coherence >= coherence_threshold
        results[ch] = {'average_tf_map': avg_tf_map, 'coherence_mask': coherence_mask}

        if logging:
            print(f"[CHANNEL {ch}] Done. % pixels above threshold: {np.sum(coherence_mask)/(L*M)*100:.1f}%")

    return results

# --- Load multi-channel audio ---
audio_file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(audio_file, sr=None, mono=False)
print("Audio shape:", y.shape)

# --- Convert each channel to TF maps ---
K = y.shape[0]
tf_maps = np.array([audio_to_tf_map(y[ch]) for ch in range(K)])
print("TF maps shape:", tf_maps.shape)

# --- Compute per-channel coherence ---
results = per_channel_coherence(tf_maps, coherence_threshold=0, logging=True)

# --- Visualization ---
for ch in range(K):
    avg_tf_map = results[ch]['average_tf_map']
    coherence_mask = results[ch]['coherence_mask']

    plt.figure(figsize=(12, 4))
    librosa.display.specshow(20*np.log10(avg_tf_map+1e-8), sr=sr, hop_length=512,
                             y_axis='log', x_axis='time')
    plt.colorbar(format='%+2.0f dB')
    plt.title(f"Channel {ch} Average TF Map")
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(12, 3))
    plt.imshow(coherence_mask, aspect='auto', origin='lower', cmap='viridis')
    plt.colorbar(label='Coherence Mask')
    plt.title(f"Channel {ch} Coherence Mask")
    plt.tight_layout()
    plt.show()

    # Overlay mask on TF map
    plt.figure(figsize=(12, 4))
    plt.imshow(20*np.log10(avg_tf_map+1e-8), aspect='auto', origin='lower', cmap='magma')
    plt.imshow(coherence_mask, aspect='auto', origin='lower', cmap='cool', alpha=0.4)
    plt.title(f"Channel {ch} TF Map with Coherence Mask Overlay")
    plt.tight_layout()
    plt.show()

import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt

# --- Convert 1D audio to TF map ---
def audio_to_tf_map(audio, n_fft=1024, hop_length=512):
    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)
    tf_map = np.abs(stft)
    return tf_map

# --- Pixel-wise coherence ---
def calc_coherence(tf1, tf2):
    tf1_norm = tf1 / (np.max(tf1, axis=0, keepdims=True) + 1e-8)
    tf2_norm = tf2 / (np.max(tf2, axis=0, keepdims=True) + 1e-8)
    coherence = (tf1_norm * tf2_norm)**2
    return np.clip(coherence, 0, 1)

# --- Process coherence for each channel separately ---
def per_channel_coherence(tf_maps: np.ndarray, coherence_threshold=0.5, logging=False):
    K, L, M = tf_maps.shape
    results = {}

    for ch in range(K):
        if logging:
            print(f"\n[CHANNEL {ch}] Processing coherence with other channels")

        # Initialize max coherence map
        max_coherence = np.zeros((L, M))
        best_partner = np.zeros((L, M), dtype=int)

        # Compute coherence with all other channels
        for other_ch in range(K):
            if other_ch == ch:
                continue
            coh_map = calc_coherence(tf_maps[ch], tf_maps[other_ch])
            update_mask = coh_map > max_coherence
            max_coherence[update_mask] = coh_map[update_mask]
            best_partner[update_mask] = other_ch

        # Compute average TF map with most coherent partner per pixel
        avg_tf_map = (tf_maps[ch] + tf_maps[best_partner, np.arange(L)[:, None], np.arange(M)]) / 2.0

        # Threshold coherence to get mask
        coherence_mask = max_coherence >= coherence_threshold
        results[ch] = {'average_tf_map': avg_tf_map, 'coherence_mask': coherence_mask}

        if logging:
            print(f"[CHANNEL {ch}] Done. % pixels above threshold: {np.sum(coherence_mask)/(L*M)*100:.1f}%")

    return results

# --- Load multi-channel audio ---
audio_file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(audio_file, sr=None, mono=False)
print("Audio shape:", y.shape)

# --- Convert each channel to TF maps ---
K = y.shape[0]
tf_maps = np.array([audio_to_tf_map(y[ch]) for ch in range(K)])
print("TF maps shape:", tf_maps.shape)

# --- Compute per-channel coherence ---
results = per_channel_coherence(tf_maps, coherence_threshold=0.2, logging=True)

# --- Visualization ---
for ch in range(K):
    avg_tf_map = results[ch]['average_tf_map']
    coherence_mask = results[ch]['coherence_mask']

    plt.figure(figsize=(12, 4))
    librosa.display.specshow(20*np.log10(avg_tf_map+1e-8), sr=sr, hop_length=512,
                             y_axis='log', x_axis='time')
    plt.colorbar(format='%+2.0f dB')
    plt.title(f"Channel {ch} Average TF Map")
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(12, 3))
    plt.imshow(coherence_mask, aspect='auto', origin='lower', cmap='viridis')
    plt.colorbar(label='Coherence Mask')
    plt.title(f"Channel {ch} Coherence Mask")
    plt.tight_layout()
    plt.show()

    # Overlay mask on TF map
    plt.figure(figsize=(12, 4))
    plt.imshow(20*np.log10(avg_tf_map+1e-8), aspect='auto', origin='lower', cmap='magma')
    plt.imshow(coherence_mask, aspect='auto', origin='lower', cmap='cool', alpha=0.4)
    plt.title(f"Channel {ch} TF Map with Coherence Mask Overlay")
    plt.tight_layout()
    plt.show()

import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt

# --- Convert 1D audio to TF map ---
def audio_to_tf_map(audio, n_fft=1024, hop_length=512):
    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)
    tf_map = np.abs(stft)
    return tf_map

# --- Pixel-wise coherence ---
def calc_coherence(tf1, tf2):
    tf1_norm = tf1 / (np.max(tf1, axis=0, keepdims=True) + 1e-8)
    tf2_norm = tf2 / (np.max(tf2, axis=0, keepdims=True) + 1e-8)
    coherence = (tf1_norm * tf2_norm)**2
    return np.clip(coherence, 0, 1)

# --- Adaptive per-channel coherence ---
def per_channel_coherence(tf_maps: np.ndarray, percentile_threshold=70, logging=False):
    """
    For each channel, compute pixel-wise coherence with all other channels.
    Threshold coherence adaptively using a percentile of max coherence values.
    """
    K, L, M = tf_maps.shape
    results = {}

    for ch in range(K):
        if logging:
            print(f"\n[CHANNEL {ch}] Processing coherence with other channels")

        # Initialize max coherence map
        max_coherence = np.zeros((L, M))
        best_partner = np.zeros((L, M), dtype=int)

        # Compute coherence with all other channels
        for other_ch in range(K):
            if other_ch == ch:
                continue
            coh_map = calc_coherence(tf_maps[ch], tf_maps[other_ch])
            update_mask = coh_map > max_coherence
            max_coherence[update_mask] = coh_map[update_mask]
            best_partner[update_mask] = other_ch

        # Compute adaptive threshold from percentile
        threshold_value = np.percentile(max_coherence, percentile_threshold)
        coherence_mask = max_coherence >= threshold_value

        # Average TF map with the most coherent partner per pixel
        avg_tf_map = (tf_maps[ch] + tf_maps[best_partner, np.arange(L)[:, None], np.arange(M)]) / 2.0

        results[ch] = {'average_tf_map': avg_tf_map,
                       'coherence_mask': coherence_mask,
                       'threshold_value': threshold_value}

        if logging:
            pct_pixels = np.sum(coherence_mask)/(L*M)*100
            print(f"[CHANNEL {ch}] Adaptive threshold: {threshold_value:.3f}, % pixels above threshold: {pct_pixels:.1f}%")

    return results

# --- Load multi-channel audio ---
audio_file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(audio_file, sr=None, mono=False)
print("Audio shape:", y.shape)

# --- Convert each channel to TF maps ---
K = y.shape[0]
tf_maps = np.array([audio_to_tf_map(y[ch]) for ch in range(K)])
print("TF maps shape:", tf_maps.shape)

# --- Compute per-channel coherence ---
results = per_channel_coherence(tf_maps, percentile_threshold=70, logging=True)

# --- Visualization ---
for ch in range(K):
    avg_tf_map = results[ch]['average_tf_map']
    coherence_mask = results[ch]['coherence_mask']
    threshold_val = results[ch]['threshold_value']

    plt.figure(figsize=(12, 4))
    librosa.display.specshow(20*np.log10(avg_tf_map+1e-8), sr=sr, hop_length=512,
                             y_axis='log', x_axis='time')
    plt.colorbar(format='%+2.0f dB')
    plt.title(f"Channel {ch} Average TF Map")
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(12, 3))
    plt.imshow(coherence_mask, aspect='auto', origin='lower', cmap='viridis')
    plt.colorbar(label='Coherence Mask')
    plt.title(f"Channel {ch} Coherence Mask (Threshold={threshold_val:.3f})")
    plt.tight_layout()
    plt.show()

    # Overlay mask on TF map
    plt.figure(figsize=(12, 4))
    plt.imshow(20*np.log10(avg_tf_map+1e-8), aspect='auto', origin='lower', cmap='magma')
    plt.imshow(coherence_mask, aspect='auto', origin='lower', cmap='cool', alpha=0.4)
    plt.title(f"Channel {ch} TF Map with Coherence Mask Overlay")
    plt.tight_layout()
    plt.show()

import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from typing import Tuple

# --- Convert 1D audio to TF map ---
def audio_to_tf_map(audio, n_fft=1024, hop_length=512):
    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)
    tf_map = np.abs(stft)
    return tf_map

# --- Pixel-wise coherence ---
def calc_coherence(tf1, tf2):
    tf1_norm = tf1 / (np.max(tf1, axis=0, keepdims=True) + 1e-8)
    tf2_norm = tf2 / (np.max(tf2, axis=0, keepdims=True) + 1e-8)
    coherence = (tf1_norm * tf2_norm)**2
    return np.clip(coherence, 0, 1)

# --- Convert pair index back to channels ---
def _pair_index_to_channels(pair_idx: int, K: int) -> Tuple[int, int]:
    i = 0
    remaining = pair_idx
    while remaining >= (K - 1 - i):
        remaining -= (K - 1 - i)
        i += 1
    j = i + 1 + remaining
    return i, j

# --- Adaptive multi-channel coherence ---
def process_multi_channel_coherence_adaptive(
        tf_maps: np.ndarray,
        calc_coherence: callable,
        percentile_threshold: float = 70,
        logging: bool = False
    ) -> Tuple[np.ndarray, np.ndarray, float]:
    """
    Process multi-channel TF maps to calculate coherence between all channel pairs.
    Uses percentile-based adaptive thresholding.
    """
    K, L, M = tf_maps.shape
    if K < 2:
        raise ValueError("At least 2 channels are required for coherence analysis")

    num_pairs = K * (K - 1) // 2
    max_coherence = np.zeros((L, M))
    best_pair_indices = np.zeros((L, M), dtype=int)

    # Calculate coherence for all channel pairs
    pair_idx = 0
    for i in range(K):
        for j in range(i + 1, K):
            coherence = calc_coherence(tf_maps[i], tf_maps[j])
            better_mask = coherence > max_coherence
            max_coherence[better_mask] = coherence[better_mask]
            best_pair_indices[better_mask] = pair_idx
            pair_idx += 1

    # Adaptive threshold using percentile
    threshold_value = np.percentile(max_coherence, percentile_threshold)
    coherence_mask = max_coherence >= threshold_value

    # Average TF map of most coherent pair per pixel
    average_tf_map = np.zeros((L, M))
    for l in range(L):
        for m in range(M):
            best_pair = best_pair_indices[l, m]
            i, j = _pair_index_to_channels(best_pair, K)
            average_tf_map[l, m] = (tf_maps[i, l, m] + tf_maps[j, l, m]) / 2.0

    if logging:
        pct_pixels = np.sum(coherence_mask) / (L * M) * 100
        print(f"[COHERENCE] Adaptive threshold (percentile {percentile_threshold}): {threshold_value:.3f}")
        print(f"[COHERENCE] % pixels above threshold: {pct_pixels:.1f}%")

    return average_tf_map, coherence_mask, threshold_value

# --- Load multi-channel audio ---
audio_file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(audio_file, sr=None, mono=False)
print("Audio shape:", y.shape)

# --- Convert each channel to TF maps ---
K = y.shape[0]
tf_maps = np.array([audio_to_tf_map(y[ch]) for ch in range(K)])
print("TF maps shape:", tf_maps.shape)

# --- Compute adaptive multi-channel coherence ---
percentile_threshold = 70  # Top 30% coherent pixels
average_tf_map, coherence_mask, threshold_val = process_multi_channel_coherence_adaptive(
    tf_maps, calc_coherence, percentile_threshold=percentile_threshold, logging=True
)

# --- Visualization ---
plt.figure(figsize=(12, 4))
librosa.display.specshow(20*np.log10(average_tf_map+1e-8), sr=sr, hop_length=512,
                         y_axis='log', x_axis='time')
plt.colorbar(format='%+2.0f dB')
plt.title("Average TF Map of Most Coherent Pair")
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 3))
plt.imshow(coherence_mask, aspect='auto', origin='lower', cmap='viridis')
plt.colorbar(label='Coherence Mask')
plt.title(f"Coherence Mask (Adaptive Threshold={threshold_val:.3f})")
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 4))
plt.imshow(20*np.log10(average_tf_map+1e-8), aspect='auto', origin='lower', cmap='magma')
plt.imshow(coherence_mask, aspect='auto', origin='lower', cmap='cool', alpha=0.4)
plt.title("TF Map with Coherence Mask Overlay")
plt.tight_layout()
plt.show()

import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from typing import Tuple

# --- Convert 1D audio to TF map ---
def audio_to_tf_map(audio, n_fft=1024, hop_length=512):
    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)
    tf_map = np.abs(stft)
    return tf_map

# --- Pixel-wise coherence ---
def calc_coherence(tf1, tf2):
    tf1_norm = tf1 / (np.max(tf1, axis=0, keepdims=True) + 1e-8)
    tf2_norm = tf2 / (np.max(tf2, axis=0, keepdims=True) + 1e-8)
    coherence = (tf1_norm * tf2_norm)**2
    return np.clip(coherence, 0, 1)

# --- Convert pair index back to channels ---
def _pair_index_to_channels(pair_idx: int, K: int) -> Tuple[int, int]:
    i = 0
    remaining = pair_idx
    while remaining >= (K - 1 - i):
        remaining -= (K - 1 - i)
        i += 1
    j = i + 1 + remaining
    return i, j

# --- Adaptive multi-channel coherence with auto-adjusted threshold ---
def process_multi_channel_coherence_adaptive(
        tf_maps: np.ndarray,
        calc_coherence: callable,
        target_mask_pct: float = 20.0,
        tolerance: float = 2.0,
        logging: bool = False
    ) -> Tuple[np.ndarray, np.ndarray, float]:

    K, L, M = tf_maps.shape
    if K < 2:
        raise ValueError("At least 2 channels are required for coherence analysis")

    num_pairs = K * (K - 1) // 2
    max_coherence = np.zeros((L, M))
    best_pair_indices = np.zeros((L, M), dtype=int)

    # Calculate coherence for all channel pairs
    pair_idx = 0
    for i in range(K):
        for j in range(i + 1, K):
            coherence = calc_coherence(tf_maps[i], tf_maps[j])
            better_mask = coherence > max_coherence
            max_coherence[better_mask] = coherence[better_mask]
            best_pair_indices[better_mask] = pair_idx
            pair_idx += 1

    # --- Auto-adjust percentile threshold ---
    percentile_threshold = 70  # initial guess
    step = 1
    max_iter = 100
    for _ in range(max_iter):
        threshold_value = np.percentile(max_coherence, percentile_threshold)
        coherence_mask = max_coherence >= threshold_value
        mask_pct = np.sum(coherence_mask) / (L * M) * 100
        if abs(mask_pct - target_mask_pct) <= tolerance:
            break
        if mask_pct > target_mask_pct:
            percentile_threshold += step  # raise threshold
        else:
            percentile_threshold -= step  # lower threshold

    # Average TF map of most coherent pair per pixel
    average_tf_map = np.zeros((L, M))
    for l in range(L):
        for m in range(M):
            best_pair = best_pair_indices[l, m]
            i, j = _pair_index_to_channels(best_pair, K)
            average_tf_map[l, m] = (tf_maps[i, l, m] + tf_maps[j, l, m]) / 2.0

    if logging:
        print(f"[COHERENCE] Auto-adjusted percentile threshold: {percentile_threshold:.1f}")
        print(f"[COHERENCE] Threshold value: {threshold_value:.3f}")
        print(f"[COHERENCE] Mask coverage: {mask_pct:.2f}%")

    return average_tf_map, coherence_mask, threshold_value

# --- Load multi-channel audio ---
audio_file = "/content/DroneSound_noMuff.wav"
y, sr = librosa.load(audio_file, sr=None, mono=False)
print("Audio shape:", y.shape)

# --- Convert each channel to TF maps ---
K = y.shape[0]
tf_maps = np.array([audio_to_tf_map(y[ch]) for ch in range(K)])
print("TF maps shape:", tf_maps.shape)

# --- Compute adaptive multi-channel coherence ---
average_tf_map, coherence_mask, threshold_val = process_multi_channel_coherence_adaptive(
    tf_maps, calc_coherence, target_mask_pct=20.0, logging=True
)

# --- Visualization ---
plt.figure(figsize=(12, 4))
plt.imshow(20*np.log10(average_tf_map+1e-8), aspect='auto', origin='lower', cmap='viridis')
plt.colorbar(format='%+2.0f dB')
plt.title("Average TF Map of Most Coherent Pair")
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 3))
plt.imshow(20*np.log10(coherence_mask), aspect='auto', origin='lower', cmap='viridis')
plt.colorbar(label='Coherence Mask')
plt.title(f"Coherence Mask (Threshold={threshold_val:.3f})")
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 4))
#plt.imshow(20*np.log10(average_tf_map+1e-8), aspect='auto', origin='lower', cmap='magma')
plt.imshow(20*np.log10(average_tf_map+1e-8)/coherence_mask, aspect='auto', origin='lower', cmap='viridis', alpha=1)
plt.title("TF Map with Coherence Mask Overlay")
plt.colorbar(label='Coherence Mask')
plt.tight_layout()
plt.show()